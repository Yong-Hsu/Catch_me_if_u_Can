{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(num_good=1, \n",
    "                        num_adversaries=3, \n",
    "                        num_obstacles=2, \n",
    "                        max_cycles=50000, \n",
    "                        continuous_actions=True, \n",
    "                        render_mode=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0 :\n",
      "(array([ 0.        ,  0.        ,  0.28114784, -0.10467174,  0.49858594,\n",
      "       -0.4031773 , -0.30692732,  0.64155465,  0.19227125,  0.14148666,\n",
      "        0.44570836,  0.3249428 ,  0.7032439 ,  0.9045745 ,  0.        ,\n",
      "        0.        ], dtype=float32), 0.0, False, False, {})\n",
      "[ 0.          0.          0.28114784 -0.10467174  0.49858594 -0.4031773\n",
      " -0.30692732  0.64155465  0.19227125  0.14148666  0.44570836  0.3249428\n",
      "  0.7032439   0.9045745   0.          0.        ]\n",
      "adversary_1 :\n",
      "(array([ 0.        ,  0.        ,  0.4734191 ,  0.03681492,  0.30631468,\n",
      "       -0.54466397, -0.4991986 ,  0.500068  , -0.19227125, -0.14148666,\n",
      "        0.2534371 ,  0.18345612,  0.5109727 ,  0.76308787,  0.        ,\n",
      "        0.        ], dtype=float32), 0.0, False, False, {})\n",
      "[ 0.          0.          0.4734191   0.03681492  0.30631468 -0.54466397\n",
      " -0.4991986   0.500068   -0.19227125 -0.14148666  0.2534371   0.18345612\n",
      "  0.5109727   0.76308787  0.          0.        ]\n",
      "adversary_2 :\n",
      "(array([ 0.        ,  0.        ,  0.72685623,  0.22027105,  0.05287758,\n",
      "       -0.7281201 , -0.7526357 ,  0.31661186, -0.44570836, -0.3249428 ,\n",
      "       -0.2534371 , -0.18345612,  0.25753558,  0.57963175,  0.        ,\n",
      "        0.        ], dtype=float32), 0.0, False, False, {})\n",
      "[ 0.          0.          0.72685623  0.22027105  0.05287758 -0.7281201\n",
      " -0.7526357   0.31661186 -0.44570836 -0.3249428  -0.2534371  -0.18345612\n",
      "  0.25753558  0.57963175  0.          0.        ]\n",
      "agent_0 :\n",
      "(array([ 0.        ,  0.        ,  0.9843918 ,  0.7999028 , -0.204658  ,\n",
      "       -1.3077519 , -1.0101713 , -0.2630199 , -0.7032439 , -0.9045745 ,\n",
      "       -0.5109727 , -0.76308787, -0.25753558, -0.57963175], dtype=float32), 0.0, False, False, {})\n",
      "[ 0.34251785  0.29751053  1.0186436   0.82965386 -0.23890978 -1.3375028\n",
      " -1.0444231  -0.29277095 -0.74664384 -0.93328774 -0.5248468  -0.79125357\n",
      " -0.2806136  -0.60326934]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "i = 0\n",
    "for agent in env.agent_iter():\n",
    "\n",
    "    print(agent, ':')\n",
    "    print(env.last())\n",
    "    # print(env.rewards[agent])\n",
    "    env.step(env.action_space(agent).sample())\n",
    "    print(env.observe(agent))\n",
    "    # observation space: shape(14),(16)\n",
    "    # Agent and adversary observations: [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities]\n",
    "    # observation function in the simple_tag.py: other_agent_rel_positions contains all agents except oneself, the order is adversary to agent,\n",
    "    # other_agent_velocities only for not adversary\n",
    "\n",
    "    i = i + 1\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71558774 0.8832722  0.9900449  0.5602077  0.03099385]\n",
      "<class 'numpy.ndarray'>\n",
      "[WARNING]: Received an action [0.0548357  0.         0.         0.02204804 0.        ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[0.6107287  0.9918348  0.98018605 0.620316   0.6496268 ]\n",
      "<class 'numpy.ndarray'>\n",
      "[WARNING]: Received an action [0.0548357  0.         0.         0.02204804 0.        ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[0.79844123 0.9156946  0.8545123  0.46651003 0.06465398]\n",
      "<class 'numpy.ndarray'>\n",
      "[WARNING]: Received an action [0.0548357  0.         0.         0.02204804 0.        ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[0.04877817 0.6622239  0.8755051  0.17754623 0.46862632]\n",
      "<class 'numpy.ndarray'>\n",
      "[WARNING]: Received an action [0.0548357  0.         0.         0.02204804 0.        ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[0.3375388  0.4168663  0.00830422 0.4935807  0.19000043]\n",
      "<class 'numpy.ndarray'>\n",
      "[WARNING]: Received an action [0.0548357  0.         0.         0.02204804 0.        ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "count = 0\n",
    "\n",
    "aa = np.array([0.0548357,  0.   ,      0.       ,  0.02204804, 0.        ])\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
    "    print(action)\n",
    "    print(type(action))\n",
    "    env.step(aa)\n",
    "    # time.sleep(0.005)\n",
    "    count = count + 1\n",
    "    if count == 5:\n",
    "        break\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems to solve: \n",
    "1. save videos [reference](https://github.com/fireofearth/cpsc533v_project/blob/c897af30921e13ac81ce01cc2a8066e1d46ecc31/colin/evaluate_tag_single_adversary.py)\n",
    "3. try DQN\n",
    "4. how to implement multiple network and train on that.\n",
    "\n",
    "4. does the agent move fixed steps by each iteration in discrete mode\n",
    "5. How to fix the landmark in fixed position, or is it just the rendering make it seem not staying where it is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action, observation and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0 :\n",
      "Box(0.0, 1.0, (5,), float32)\n",
      "True\n",
      "sample action: [0.92901367 0.37396044 0.6046699  0.68962854 0.8919782 ]\n",
      "adversary_1 :\n",
      "Box(0.0, 1.0, (5,), float32)\n",
      "False\n",
      "sample action: [0.09669605 0.5639015  0.89365935 0.8282331  0.08617229]\n",
      "adversary_2 :\n",
      "Box(0.0, 1.0, (5,), float32)\n",
      "False\n",
      "sample action: [0.13027631 0.01165134 0.27997    0.39355087 0.16513759]\n",
      "agent_0 :\n",
      "Box(0.0, 1.0, (5,), float32)\n",
      "False\n",
      "sample action: [5.1456806e-04 9.9431151e-01 3.1835628e-01 4.3276712e-01 5.6209779e-01]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "\n",
    "# action space with four directions and stay\n",
    "\n",
    "i = 0\n",
    "# sample for different agents\n",
    "for agent in env.agent_iter():\n",
    "    a = env.action_space(agent).sample()\n",
    "    print(agent, ':')\n",
    "    # [no_action, move_left, move_right, move_down, move_up]\n",
    "    print(env.action_space(agent))\n",
    "    print(agent == 'adversary_0')\n",
    "    env.step(a)\n",
    "    print('sample action:', a)\n",
    "    # print(env.observation_spaces)\n",
    "    \n",
    "    i = i + 1\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15303998,  0.03050993, -0.97915745, -0.28953764,  1.5197377 ,\n",
       "        1.0614314 ,  1.2278101 ,  0.28194538,  0.6069736 ,  0.8174495 ,\n",
       "        0.44835332,  0.46482474,  0.5754395 ,  1.1692449 ,  0.01615815,\n",
       "       -0.00480299, -0.09565483,  0.11386324, -0.3721839 ,  0.5279119 ,\n",
       "        0.9127642 ,  0.24398187,  0.6208366 , -0.53550416, -0.6069736 ,\n",
       "       -0.8174495 , -0.15862025, -0.3526248 , -0.03153404,  0.35179535,\n",
       "        0.01615815, -0.00480299, -0.14904232, -0.03848771, -0.53080416,\n",
       "        0.17528708,  1.0713844 ,  0.5966067 ,  0.77945685, -0.18287934,\n",
       "       -0.44835332, -0.46482474,  0.15862025,  0.3526248 ,  0.1270862 ,\n",
       "        0.70442015,  0.01615815, -0.00480299,  0.01615815, -0.00480299,\n",
       "       -0.40371794,  0.8797072 ,  0.94429827, -0.10781348,  0.65237063,\n",
       "       -0.8872995 , -0.5754395 , -1.1692449 ,  0.03153404, -0.35179535,\n",
       "       -0.1270862 , -0.70442015], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state, should be 62?\n",
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(num_good=1, \n",
    "                        num_adversaries=3, \n",
    "                        num_obstacles=2, \n",
    "                        max_cycles=50, \n",
    "                        continuous_actions=True, \n",
    "                        render_mode=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0 :\n",
      "[ 0.          0.         -0.05791276  0.3346408  -0.8086802   0.06578394\n",
      "  0.7563633  -0.31174925 -0.6584351  -1.1056974   0.22030357 -1.2653029\n",
      "  0.9858129  -0.24132617  0.          0.        ]\n",
      "adversary_1 :\n",
      "[ 0.          0.         -0.7163479  -0.77105653 -0.15024512  1.1714813\n",
      "  1.4147984   0.79394805  0.6584351   1.1056974   0.87873864 -0.15960555\n",
      "  1.644248    0.8643712   0.          0.        ]\n",
      "adversary_2 :\n",
      "[ 0.          0.          0.1623908  -0.9306621  -1.0289837   1.3310869\n",
      "  0.53605974  0.9535536  -0.22030357  1.2653029  -0.87873864  0.15960555\n",
      "  0.7655093   1.0239767   0.          0.        ]\n",
      "agent_0 :\n",
      "[ 0.16266285 -0.12010477  0.9441664   0.08130416 -1.8107594   0.3191206\n",
      " -0.24571589 -0.05841262 -1.0147394   0.24549443 -1.6538827  -0.8703886\n",
      " -0.7812546  -1.0105222 ]\n"
     ]
    }
   ],
   "source": [
    "# env.reset()\n",
    "# obj = []\n",
    "# i = 0\n",
    "# for agent in env.agent_iter():\n",
    "#\n",
    "#     obj.append(agent)\n",
    "#     env.step([1,1,1,1,1])\n",
    "#     i = i + 1\n",
    "#     if i == 4:\n",
    "#         break\n",
    "\n",
    "env.reset()\n",
    "i = 0\n",
    "for agent in env.agent_iter():\n",
    "\n",
    "    print(agent, ':')\n",
    "    env.step(env.action_space(agent).sample())\n",
    "    # observation space: shape(14),(16)\n",
    "    # Agent and adversary observations: [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities]\n",
    "    # observation function in the simple_tag.py: other_agent_rel_positions contains all agents except oneself, the order is adversary to agent, \n",
    "    # other_agent_velocities only for not adversary\n",
    "    print(env.last())\n",
    "    \n",
    "    i = i + 1\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
