{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(num_good=1, \n",
    "                        num_adversaries=3, \n",
    "                        num_obstacles=2, \n",
    "                        max_cycles=100, \n",
    "                        continuous_actions=True, \n",
    "                        render_mode=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
    "    \n",
    "    env.step(action)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: \n",
    "1. save videos [reference](https://github.com/fireofearth/cpsc533v_project/blob/c897af30921e13ac81ce01cc2a8066e1d46ecc31/colin/evaluate_tag_single_adversary.py)\n",
    "3. try DQN\n",
    "4. how to implement multiple network and train on that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action, observation and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 1.0, (5,), float32)\n",
      "adversary_0 :\n",
      "sample action: [0.0119764  0.252828   0.18079923 0.7381237  0.6851406 ]\n",
      "adversary_1 :\n",
      "sample action: [0.10676163 0.53216404 0.11538256 0.9378003  0.21284239]\n",
      "adversary_2 :\n",
      "sample action: [0.01342471 0.95402396 0.5021012  0.8891766  0.65971637]\n",
      "agent_0 :\n",
      "sample action: [0.19853032 0.1248109  0.7256598  0.19498755 0.21039023]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "\n",
    "# action space with four directions and stay\n",
    "# [no_action, move_left, move_right, move_down, move_up]\n",
    "print(env.action_space(agent))\n",
    "\n",
    "i = 0\n",
    "# sample for different agents\n",
    "for agent in env.agent_iter():\n",
    "    a = env.action_space(agent).sample()\n",
    "    print(agent, ':')\n",
    "    env.step(a)\n",
    "    print('sample action:', a)\n",
    "    # print(env.observation_spaces)\n",
    "    \n",
    "    i = i + 1\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00,  0.0000000e+00, -8.5962892e-01,  8.8099217e-01,\n",
       "        1.0135393e+00, -4.1221848e-01,  3.3933982e-01, -1.7276236e+00,\n",
       "        9.8698556e-01, -9.8570061e-01,  8.6008966e-01, -1.6648449e-01,\n",
       "        1.0217097e+00, -1.6673965e+00,  0.0000000e+00,  4.0000001e-01,\n",
       "       -3.0000001e-01, -0.0000000e+00,  1.2735660e-01, -1.0470843e-01,\n",
       "        2.6553761e-02,  5.7348216e-01, -6.4764571e-01, -7.4192303e-01,\n",
       "       -9.8698556e-01,  9.8570061e-01, -1.2689586e-01,  8.1921613e-01,\n",
       "        3.4724202e-02, -6.8169594e-01,  0.0000000e+00,  4.0000001e-01,\n",
       "       -5.2957845e-01,  4.5698982e-01,  4.6074370e-04,  7.1450770e-01,\n",
       "        1.5344962e-01, -2.4573398e-01, -5.2074987e-01, -1.5611391e+00,\n",
       "       -8.6008966e-01,  1.6648449e-01,  1.2689586e-01, -8.1921613e-01,\n",
       "        1.6162007e-01, -1.5009121e+00,  0.0000000e+00,  4.0000001e-01,\n",
       "        0.0000000e+00,  4.0000001e-01,  1.6208081e-01, -7.8640437e-01,\n",
       "       -8.1704417e-03,  1.2551781e+00, -6.8236995e-01, -6.0227096e-02,\n",
       "       -1.0217097e+00,  1.6673965e+00, -3.4724202e-02,  6.8169594e-01,\n",
       "       -1.6162007e-01,  1.5009121e+00], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state, should be 62?\n",
    "env.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0 :\n",
      "[ 0.          0.         -0.34912184  0.5028731  -0.44968572 -0.1900941\n",
      "  0.9063153  -0.38171014 -0.6211034   0.0766574   0.72543776 -0.48805845\n",
      "  1.1138225  -1.4590774   0.          0.        ]\n",
      "adversary_1 :\n",
      "[ 0.          0.         -0.9702253   0.57953054  0.17141771 -0.2667515\n",
      "  1.5274187  -0.45836753  0.6211034  -0.0766574   1.3465412  -0.56471586\n",
      "  1.7349259  -1.5357348   0.          0.        ]\n",
      "adversary_2 :\n",
      "[ 0.          0.          0.37631595  0.0148147  -1.1751235   0.29796433\n",
      "  0.18087757  0.10634831 -0.72543776  0.48805845 -1.3465412   0.56471586\n",
      "  0.38838464 -0.9710189   0.          0.        ]\n",
      "agent_0 :\n",
      "[ 0.         -0.4         0.7647006  -0.9962042  -1.5635082   1.3089832\n",
      " -0.20750709  1.1173673  -1.0838225   1.4990773  -1.7349259   1.6057347\n",
      " -0.47456777  0.9779856 ]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "\n",
    "i = 0\n",
    "for agent in env.agent_iter():\n",
    "\n",
    "    print(agent, ':')\n",
    "    env.step(env.action_space(agent).sample())\n",
    "    # observation space: shape(14),(16)\n",
    "    # Agent and adversary observations: [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities]\n",
    "    print(env.observe(agent))\n",
    "    a = env.observe(agent)\n",
    "    \n",
    "    i = i + 1\n",
    "    if i == 4:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [self_vel, self_pos, landmark_rel_positions, other_agent_rel_positions, other_agent_velocities]\n",
    "# all of them has two elements\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# figure out how many networks\n",
    "# start from basic code from dqn\n",
    "# read the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound(x):\n",
    "    if x < 0.9:\n",
    "        return 0\n",
    "    if x < 1.0:\n",
    "        return (x - 0.9) * 10\n",
    "    return min(np.exp(2 * x - 2), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN, self).__init__()\n",
    "        # network\n",
    "        self.out = nn.Linear(n_inputs, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out(x)\n",
    "    \n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "    \n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
