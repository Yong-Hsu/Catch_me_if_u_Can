{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = simple_tag_v2.env(\n",
    "            num_good=1,\n",
    "            num_adversaries=3,\n",
    "            num_obstacles=2,\n",
    "            max_cycles=10000,\n",
    "            continuous_actions=False,\n",
    "            # render_mode='human'\n",
    "        )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define an experience replay memory that can be used to store new transitions and sample mini-batches of previous transitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Experience Replay Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        #self.size = size\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        \"\"\"Add experience to memory.\"\"\"\n",
    "        self.memory.append([*args])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch of experiences from memory with replacement.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def count(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-network is very similar to the one we have seen previously, but we add the possibility to update the parameters, so the same class can also be used as a target network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN_prey(nn.Module):\n",
    "    \"\"\"Deep Q-network with target network\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, learning_rate):\n",
    "        super(DQN_prey, self).__init__()\n",
    "        # network\n",
    "        self.out = nn.Linear(n_inputs, n_outputs)\n",
    "        # training\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = x.to(device)\n",
    "        return self.out(x)\n",
    "    \n",
    "    def loss(self, q_outputs, q_targets):\n",
    "        return torch.sum(torch.pow(q_targets - q_outputs, 2))\n",
    "    \n",
    "    def update_params(self, new_params, tau):\n",
    "        params = self.state_dict()\n",
    "        for k in params.keys():\n",
    "            params[k] = (1-tau) * params[k] + tau * new_params[k]\n",
    "        self.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we create a policy network and copy its weight parameters to a target network, so they are initially the same. \n",
    "We also set up a replay memory and prefill it with random transitions sampled from the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefill replay memory ...\n",
      "prefill replay memory done\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300 # training loops\n",
    "episode_limit = 200 # \n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99 # discount rate\n",
    "tau = 0.01 # target network update rate\n",
    "replay_memory_capacity = 5000\n",
    "prefill_memory = True\n",
    "val_freq = 100 # validation frequency\n",
    "\n",
    "# n_inputs = env.observation_space.n\n",
    "# n_outputs = env.action_space.n\n",
    "n_inputs = 14\n",
    "n_outputs = 5\n",
    "\n",
    "# initialize DQN and replay memory\n",
    "policy_dqn = DQN_prey(n_inputs, n_outputs, learning_rate)\n",
    "target_dqn = DQN_prey(n_inputs, n_outputs, learning_rate)\n",
    "# .to(device)\n",
    "target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "replay_memory = ReplayMemory(replay_memory_capacity)\n",
    "\n",
    "# prefill replay memory with random actionss\n",
    "env.reset()\n",
    "if prefill_memory:\n",
    "    print('prefill replay memory ...')\n",
    "    \n",
    "    s = None\n",
    "    s1 = None\n",
    "    r = None\n",
    "\n",
    "    count = 1\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        if count == 4:\n",
    "            s = env.last()[0]\n",
    "        \n",
    "        if replay_memory.count() >= replay_memory_capacity:\n",
    "            break\n",
    "\n",
    "        s1, reward, termination, truncation, _ = env.last()\n",
    "        action = env.action_space(agent).sample() \n",
    "        env.step(action)\n",
    "\n",
    "        if count % 4 == 0:\n",
    "            replay_memory.add(s, action, reward, s1, termination)\n",
    "            # print(reward)\n",
    "\n",
    "        if not (termination or truncation):\n",
    "            if count % 4 == 0:\n",
    "                s = s1\n",
    "        else:\n",
    "            env.reset()\n",
    "            count = 1\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print('prefill replay memory done')\n",
    "\n",
    "# test 1\n",
    "# print(one_hot([0,1,2], 14))\n",
    "# policy_dqn(torch.from_numpy(one_hot([0, 1, 2] ,14)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 101/300 [00:13<00:30,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 mean training reward: -36.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 201/300 [00:29<00:15,  6.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  200 mean training reward: -36.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:42<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  300 mean training reward: -52.42\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "env.reset()\n",
    "\n",
    "try:\n",
    "    print('start training')\n",
    "    epsilon = 0.5\n",
    "    rewards, lengths, losses, epsilons = [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        # init new episode\n",
    "        for agent in env.agent_iter():\n",
    "            if agent != 'agent_0':\n",
    "                # try network only for the agent, sample action for adversary\n",
    "                env.step(env.action_space(agent).sample())\n",
    "            else:     \n",
    "                s, ep_reward, ep_loss = env.last()[0], 0, 0\n",
    "\n",
    "                for j in range(episode_limit):\n",
    "                    if env.last()[2] or env.last()[3]: \n",
    "                        env.reset()\n",
    "                        break\n",
    "\n",
    "                    if (j + 1) % 4 == 0:\n",
    "                        if np.random.rand() < epsilon:\n",
    "                            a = env.action_space(agent).sample()\n",
    "                        else:\n",
    "                            with torch.no_grad():\n",
    "                                a = policy_dqn(torch.from_numpy(s).float()).argmax().item()\n",
    "                        # perform action\n",
    "                        env.step(a)\n",
    "                    else:\n",
    "                        # print(j)\n",
    "                        env.step(0)\n",
    "                        continue\n",
    "                    # if env.last()[0].shape[0] == 14:\n",
    "\n",
    "                    s1, reward, termination, truncation, _ = env.last()\n",
    "\n",
    "                    # store experience in replay memory\n",
    "                    replay_memory.add(s, a, reward, s1, termination)\n",
    "                    \n",
    "                    # batch update\n",
    "                    if replay_memory.count() >= batch_size:\n",
    "                        # sample batch from replay memory, this is used as to predict the values in the q-table\n",
    "                        # frozen lake do one hot encoding for states, we directly put the 14-sized vector into the network\n",
    "                        batch = replay_memory.sample(batch_size)\n",
    "                        # ss, aa, rr, ss1, dd = batch[:,0], batch[:,1], batch[:,2], batch[:,3], batch[:,4]\n",
    "                        ss = np.array([list(memory[0]) for memory in batch])\n",
    "                        aa = np.array([memory[1] for memory in batch])\n",
    "                        rr = np.array([memory[2] for memory in batch])\n",
    "                        ss1 = np.array([list(memory[3]) for memory in batch])\n",
    "                        dd = np.array([memory[4] for memory in batch])\n",
    "\n",
    "                        # do forward pass of batch\n",
    "                        policy_dqn.optimizer.zero_grad()\n",
    "                        Q = policy_dqn(torch.from_numpy(ss).float())\n",
    "\n",
    "                        # use target network to compute target Q-values\n",
    "                        with torch.no_grad():\n",
    "                            Q1 = target_dqn(torch.from_numpy(ss1).float())\n",
    "                        # compute target for each sampled experience\n",
    "                        q_targets = Q.clone()\n",
    "                        for k in range(batch_size):\n",
    "                            q_targets[k, aa[k]] = rr[k] + gamma * Q1[k].max().item() * (not dd[k])\n",
    "                        \n",
    "                        # update network weights\n",
    "                        loss = policy_dqn.loss(Q, q_targets)\n",
    "                        loss.backward()\n",
    "                        policy_dqn.optimizer.step()\n",
    "                        # update target network parameters from policy network parameters\n",
    "                        target_dqn.update_params(policy_dqn.state_dict(), tau)\n",
    "\n",
    "                    else:\n",
    "                        loss = 0\n",
    "                    \n",
    "                    # bookkeeping\n",
    "                    s = s1\n",
    "                    ep_reward += reward\n",
    "                    ep_loss += loss.item()\n",
    "\n",
    "                    if termination or truncation: \n",
    "                        env.reset()\n",
    "                        break\n",
    "                \n",
    "                # bookkeeping\n",
    "                epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon\n",
    "                epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)\n",
    "\n",
    "                if (i+1) % val_freq == 0: print('%5d mean training reward: %5.2f' % (i+1, np.mean(rewards[-val_freq:])))\n",
    "            break\n",
    "    print('done')\n",
    "except KeyboardInterrupt:\n",
    "    print('interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working yet\n",
    "# plot results\n",
    "def moving_average(a, n=10) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / n\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.subplot(411)\n",
    "plt.title('training rewards')\n",
    "plt.plot(range(1, num_episodes+1), rewards)\n",
    "plt.plot(moving_average(rewards))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(412)\n",
    "plt.title('training lengths')\n",
    "plt.plot(range(1, num_episodes+1), lengths)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(lengths))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(413)\n",
    "plt.title('training loss')\n",
    "plt.plot(range(1, num_episodes+1), losses)\n",
    "plt.plot(range(1, num_episodes+1), moving_average(losses))\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.subplot(414)\n",
    "plt.title('epsilon')\n",
    "plt.plot(range(1, num_episodes+1), epsilons)\n",
    "plt.xlim([0, num_episodes])\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = simple_tag_v2.env(\n",
    "            num_good=1,\n",
    "            num_adversaries=3,\n",
    "            num_obstacles=2,\n",
    "            max_cycles=1000,\n",
    "            continuous_actions=False,\n",
    "            render_mode='human'\n",
    "        )\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if agent == 'agent_0':\n",
    "        if termination or truncation:\n",
    "            env.reset()\n",
    "            continue\n",
    "\n",
    "        action = policy_dqn(torch.from_numpy(env.last()[0]).float()).argmax().item()\n",
    "    else:\n",
    "        action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
    "    \n",
    "    env.step(action)\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
